# -*- coding: utf-8 -*-
"""Drowsy Driver

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z46soHYK8jAy9Pf5xfmcXlFc2YST41DO
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %ls
# %cd drive
# %ls

# Commented out IPython magic to ensure Python compatibility.
# %ls



import numpy as np
import pandas as pd
import os
import tensorflow as tf

from glob import glob
from tqdm import tqdm

import matplotlib.pyplot as plt
from PIL import Image

import numpy as np
from tqdm import tqdm
from glob import glob
from PIL import Image
from sklearn.model_selection import train_test_split
import os

# Base path to the dataset
base_path = '/content/drive/MyDrive/idp.dat/train'

# Verify the directory structure
print("Contents of the base directory:")
print(os.listdir(base_path))

# Paths to your Open Eyes and Closed Eyes folders (adjust if necessary)
open_eyes_path = os.path.join(base_path, 'Open_Eyes', '*.jpg')  # Adjust extension if needed
closed_eyes_path = os.path.join(base_path, 'Closed_Eyes', '*.jpg')  # Adjust extension if needed

# Ensure that the paths contain valid images
open_eye_files = glob(open_eyes_path)
closed_eye_files = glob(closed_eyes_path)

# Check if files are being loaded correctly
print(f"Number of Open Eye Images: {len(open_eye_files)}")
print(f"Number of Closed Eye Images: {len(closed_eye_files)}")

# Load and preprocess Open Eyes images
X = []
Y = []

for file in tqdm(open_eye_files, desc="Loading Open Eyes Images"):
    try:
        img = Image.open(file).resize((64, 64))  # Resize to 64x64
        img_array = np.array(img)  # Convert image to NumPy array
        X.append(img_array)
        Y.append(1)  # Label for Open Eyes
    except Exception as e:
        print(f"Error loading {file}: {e}")

# Load and preprocess Closed Eyes images
for file in tqdm(closed_eye_files, desc="Loading Closed Eyes Images"):
    try:
        img = Image.open(file).resize((64, 64))  # Resize to 64x64
        img_array = np.array(img)  # Convert image to NumPy array
        X.append(img_array)
        Y.append(0)  # Label for Closed Eyes
    except Exception as e:
        print(f"Error loading {file}: {e}")

# Verify the total number of images loaded
print(f"Total images loaded: {len(X)}")

# Convert lists to NumPy arrays
X = np.array(X)
Y = np.array(Y)

# Ensure that data was loaded
if len(X) == 0 or len(Y) == 0:
    raise ValueError("No data loaded. Check your file paths and formats.")

# Split the dataset into training and test sets
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)

# Print the sizes of the training and test sets
print(f"Training set size: {len(x_train)}")
print(f"Test set size: {len(x_test)}")

import os

# Base path to your dataset
base_path = '/content/drive/MyDrive/idp.dat/train'

# List all files and directories in the base directory
print("Contents of the base directory:")
print(os.listdir(base_path))

# List files in Open_Eyes directory
open_eyes_dir = os.path.join(base_path, 'Open_Eyes')
closed_eyes_dir = os.path.join(base_path, 'Closed_Eyes')

print("Contents of Open Eyes directory:")
print(os.listdir(open_eyes_dir))

print("Contents of Closed Eyes directory:")
print(os.listdir(closed_eyes_dir))

# Use glob to match all image files in the directory
open_eyes_path = os.path.join(base_path, 'Open_Eyes', '*.*')  # Adjust extension if needed
closed_eyes_path = os.path.join(base_path, 'Closed_Eyes', '*.*')  # Adjust extension if needed

# Check the paths used for glob
print("Open Eyes Path:", open_eyes_path)
print("Closed Eyes Path:", closed_eyes_path)

# Load the image files
open_eye_files = glob(open_eyes_path)
closed_eye_files = glob(closed_eyes_path)

# Check how many files were found
print(f"Number of Open Eye Images: {len(open_eye_files)}")
print(f"Number of Closed Eye Images: {len(closed_eye_files)}")

import numpy as np
from tqdm import tqdm
from glob import glob
from PIL import Image
from sklearn.model_selection import train_test_split
import os

# Base path to your dataset
base_path = '/content/drive/MyDrive/idp.dat/train'

# Verify the directory structure
print("Contents of the base directory:")
print(os.listdir(base_path))

# Paths to your Open Eyes and Closed Eyes folders
open_eyes_path = os.path.join(base_path, 'Open_Eyes', '*.*')  # Matches any file extension
closed_eyes_path = os.path.join(base_path, 'Closed_Eyes', '*.*')  # Matches any file extension

# Load image files
open_eye_files = glob(open_eyes_path)
closed_eye_files = glob(closed_eyes_path)

# Check if files are being loaded correctly
print(f"Number of Open Eye Images: {len(open_eye_files)}")
print(f"Number of Closed Eye Images: {len(closed_eye_files)}")

# Load and preprocess Open Eyes images
X = []
Y = []

for file in tqdm(open_eye_files, desc="Loading Open Eyes Images"):
    try:
        img = Image.open(file).resize((64, 64))  # Resize to 64x64
        img_array = np.array(img)  # Convert image to NumPy array
        X.append(img_array)
        Y.append(1)  # Label for Open Eyes
    except Exception as e:
        print(f"Error loading {file}: {e}")

# Load and preprocess Closed Eyes images
for file in tqdm(closed_eye_files, desc="Loading Closed Eyes Images"):
    try:
        img = Image.open(file).resize((64, 64))  # Resize to 64x64
        img_array = np.array(img)  # Convert image to NumPy array
        X.append(img_array)
        Y.append(0)  # Label for Closed Eyes
    except Exception as e:
        print(f"Error loading {file}: {e}")

# Verify the total number of images loaded
print(f"Total images loaded: {len(X)}")

# Convert lists to NumPy arrays
X = np.array(X)
Y = np.array(Y)

# Ensure that data was loaded
if len(X) == 0 or len(Y) == 0:
    raise ValueError("No data loaded. Check your file paths and formats.")

# Split the dataset into training and test sets
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)

# Print the sizes of the training and test sets
print(f"Training set size: {len(x_train)}")
print(f"Test set size: {len(x_test)}")



X = np.array(X)
X = X/255.0
Y = np.array(Y)

X = np.expand_dims(X,-1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Conv2D, BatchNormalization, MaxPooling2D,Dropout, Flatten

model = tf.keras.models.Sequential([
      Input(shape=(64, 64, 1)),

      Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu'),
      Conv2D(filters = 32, kernel_size = 5, strides = 1, activation = 'relu', use_bias=False),
      BatchNormalization(),
      MaxPooling2D(strides = 2),
      Dropout(0.3),

      Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu'),
      Conv2D(filters = 64, kernel_size = 3, strides = 1, activation = 'relu', use_bias=False),
      BatchNormalization(),
      MaxPooling2D(strides = 2),
      Dropout(0.3),

      Flatten(),
      Dense(units  = 256, activation = 'relu', use_bias=False),
      BatchNormalization(),

      Dense(units = 128, use_bias=False, activation = 'relu'),
 Dense(units = 84, use_bias=False, activation = 'relu'),
      BatchNormalization(),
      Dropout(0.3),

      Dense(units = 1, activation = 'sigmoid')
  ])

model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])

import tensorflow as tf

# Define the model checkpoint callback
callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='bestModel.keras',  # Update this to use .keras extension
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1
)

# Path to save the model
model_save_path = '/content/drive/MyDrive/IDP_Dataset/idp.csv/train/Closed_Eyes/bestModel.h5'

# Save the model
model.save(model_save_path)

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import os

# Create a directory for saving the model if it doesn't exist
model_dir = '/content/models'
os.makedirs(model_dir, exist_ok=True)

# Sample model definition using Input layer
model = models.Sequential([
    layers.Input(shape=(64, 64, 3)),  # Specify input shape with an Input layer
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Binary classification
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the model checkpoint callback with .keras extension
callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=os.path.join(model_dir, 'bestModel.keras'),  # Save in .keras format
    save_weights_only=False,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1
)

# Example training data (replace these with your actual datasets)
x_train = np.random.rand(100, 64, 64, 3)  # Dummy data: 100 samples, 64x64 RGB images
y_train = np.random.randint(0, 2, 100)  # Dummy labels: binary labels for 100 samples
x_val = np.random.rand(20, 64, 64, 3)    # Dummy validation data
y_val = np.random.randint(0, 2, 20)      # Dummy validation labels

# Train the model (adjust epochs and batch_size as needed)
model.fit(
    x_train,
    y_train,
    epochs=10,
    batch_size=32,
    validation_data=(x_val, y_val),
    callbacks=[callback]  # Include the ModelCheckpoint callback
)

print("x_test shape:", x_test.shape)

# Example for converting grayscale to RGB
x_test = np.stack([x_test] * 3, axis=-1)  # Repeat the single channel 3 times

# Save the model (ensure this has been executed successfully)
model.save('/content/drive/MyDrive/your_folder/bestModel.h5')  # Or .h5 if you're using that format

# Load the model with the correct path and name
best_model = load_model('/content/drive/MyDrive/your_folder/bestModel.h5')  # Update if you used a different name

import os
import numpy as np
from keras.preprocessing.image import img_to_array, load_img

# Set the main folder path
main_folder_path = '/path/to/IDP_Dataset/'  # Update this to your actual path

# Initialize a list to store image data
x_data = []

# Iterate through each subfolder
for subdir, dirs, files in os.walk(main_folder_path):
    for file in files:
        # Construct the full file path
        file_path = os.path.join(subdir, file)

        # Load the image (assuming you want to load all images)
        img = load_img(file_path, color_mode='grayscale')  # Adjust color_mode as needed
        img_array = img_to_array(img)

        # Optionally resize or preprocess the image array if needed
        # img_array = ... # Add any resizing/preprocessing here

        # Append the image array to the list
        x_data.append(img_array)

# Convert the list to a NumPy array
x_data = np.array(x_data)

# Now x_data contains all the images loaded from the subfolders

from keras.models import load_model
best_model = load_model('/content/drive/MyDrive/your_folder/bestModel.h5')  # Or use .keras if you saved in Keras format

best_model.summary()  # This will print the model layers and configuration

import os
file_path = '/content/drive/MyDrive/your_folder/bestModel.h5'
if os.path.exists(file_path):
    print("File exists")
else:
    print("File not found")

import matplotlib.pyplot as plt
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from keras.preprocessing.image import img_to_array, load_img

# ... (Your model definition and loading code) ...

for i in x_test[0:5]:
    # Convert the image to grayscale
    gray_img = np.dot(i[..., :3], [0.299, 0.587, 0.114])

    # Reshape to (1, 64, 64, 1) for grayscale and ensure data type is float32
    img = gray_img.reshape(1, 64, 64, 1).astype('float32')

    result = best_model.predict(img)

    # Display the image (using the original RGB image for visualization)
    plt.imshow(i.squeeze())  # Remove cmap='gray' as we are displaying the RGB image
    plt.show()

    # Assuming you want to check if any prediction in the result is greater than 0.5
    if (result > 0.5).any():  # Use .any() to check if any element is True
        print('Open')
    else:
        print("Closed")

import numpy as np

# Remove the extra dimension (assuming it's color channels)
# and reshape to (number_of_samples, 64, 64, 3) for RGB
x_test_reshaped = x_test.reshape(x_test.shape[0], 64, 64, 3)

# Assuming your images are in RGB format and you need to convert them to grayscale
x_test_gray = np.dot(x_test_reshaped[..., :3], [0.299, 0.587, 0.114])

# Reshape to (number_of_samples, 64, 64, 1) for grayscale
x_test_gray = x_test_gray.reshape(x_test_gray.shape[0], 64, 64, 1)

# Ensure the data type is float32
x_test_gray = x_test_gray.astype('float32')

# Now use x_test_gray for prediction
preds = best_model.predict(x_test_gray)

